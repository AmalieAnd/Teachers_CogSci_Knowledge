---
title: "Computational Modeling - Week 5 - Assignment 2 - Part 2"
author: "Riccardo Fusaroli"
date: "2/19/2017"
output: html_document
---

## In this assignment we learn how to assess rates from a binomial distribution, using the case of assessing your teachers' knowledge of CogSci.

### Second part: Focusing on predictions

Last year you assessed the teachers (darned time runs quick!). Now you want to re-test them and assess whether your models are producing reliable predictions. In Methods 3 we learned how to do machine-learning style assessment of predictions (e.g. rmse on testing datasets). Bayesian stats makes things a bit more complicated. So we'll try out how that works. N.B. You can choose which prior to use for the analysis of last year's data.

Questions to be answered (but see guidance below):
  1. Write a paragraph discussing how assessment of prediction performance is different in Bayesian vs. frequentist models
  2. Provide at least one plot and one written line discussing prediction errors for each of the teachers.

This is the old data:
- Riccardo: 3 correct answers out of 6 questions
- Kristian: 2 correct answers out of 2 questions (then he gets bored)
- Josh: 160 correct answers out of 198 questions (Josh never gets bored)
- Mikkel: 66 correct answers out of 132 questions

This is the new data:
- Riccardo: 9 correct answers out of 10 questions (then he freaks out about teaching preparation and leaves)
- Kristian: 8 correct answers out of 12 questions
- Josh: 148 correct answers out of 172 questions (again, Josh never gets bored)
- Mikkel: 34 correct answers out of 65 questions

Guidance Tips

1. There are at least two ways of assessing predictions.
2. Last year's results are this year's expectations.
3. Are the parameter estimates changing? (way 1)
4. How does the new data look in last year's predictive posterior? (way 2)

### Sampling 
```{r define grid}
#define grid
p_grid <- seq(from= 0, to = 1, length.out = 200) 
```

```{r ric}

#- Riccardo: 3 correct answers out of 6 questions
#- Riccardo: 9 correct answers out of 10 questions (then he freaks out about teaching preparation and leaves)

prior_ric <- dnorm(p_grid, mean = 0.8, sd = 0.2)

#compute likelihood at each value in grid
likelihood_ric <- dbinom(3, size = 6, prob = p_grid)

#compute product of likelihood and prior
unstd.posterior_ric <- likelihood_ric * prior_ric

#standardise the posterior, so it sums to 1
prior_ric <- unstd.posterior_ric / sum(unstd.posterior_ric)

set.seed(69)
samples_ric <- sample(p_grid, prob = prior_ric, size=1e4 , replace=TRUE)
plot(samples_ric)
dens(samples_ric)

# posterior predictive distribution
ppd_r <- rbinom( 1e4 , size=10 , prob=samples_ric )
simplehist(ppd_r, xlab = "riccardo predicted answers")

# how does the first posterior miss
simplehist(ppd_r - 9)
```

```{r kris}
#- Kristian: 2 correct answers out of 2 questions (then he gets bored)
#- Kristian: 8 correct answers out of 12 questions

prior_kris <- dnorm(p_grid, mean = 0.8, sd = 0.2)

#compute likelihood at each value in grid
likelihood_kris <- dbinom(2, size = 2, prob = p_grid)

#compute product of likelihood and prior
unstd.posterior_kris <- likelihood_kris * prior_kris 

#standardise the posterior, so it sums to 1
prior_kris <- unstd.posterior_kris / sum(unstd.posterior_kris)

set.seed(69)
samples_kris <- sample(p_grid, prob = prior_kris, size=1e4 , replace=TRUE)
plot(samples_kris)
dens(samples_kris) 

# posterior predictive distribution
ppd_k <- rbinom( 1e4 , size=12 , prob=samples_kris )
simplehist(ppd_k, xlab = "kristian predicted answers")

simplehist(ppd_k - 8)
```

```{r Josh}
#- Josh: 160 correct answers out of 198 questions (Josh never gets bored)
#- Josh: 148 correct answers out of 172 questions (again, Josh never gets bored)

prior_josh <- dnorm(p_grid, mean = 0.8, sd = 0.2)

#compute likelihood at each value in grid
likelihood_josh <- dbinom(160, size = 198, prob = p_grid)

#compute product of likelihood and prior
unstd.posterior_josh <- likelihood_josh * prior_josh
 
#standardise the posterior, so it sums to 1
prior_josh <- unstd.posterior_josh / sum(unstd.posterior_josh)

set.seed(69)
samples_josh <- sample(p_grid, prob = prior_josh, size=1e4 , replace=TRUE)
plot(samples_josh)
dens(samples_josh)

# posterior predictive distribution
ppd_j <- rbinom( 1e4 , size=172 , prob=samples_josh )
simplehist(ppd_j, xlab = "josh predicted answers")

simplehist(ppd_j - 148)
```

```{r mik}
#- Mikkel: 66 correct answers out of 132 questions
#- Mikkel: 34 correct answers out of 65 questions

prior_mik <- dnorm(p_grid, mean = 0.8, sd = 0.2)
 
#compute likelihood at each value in grid
likelihood_mik <- dbinom(66, size = 132, prob = p_grid)

#compute product of likelihood and prior
unstd.posterior_mik <- likelihood_mik * prior_mik

#standardise the posterior, so it sums to 1
prior_mik <- unstd.posterior_mik / sum(unstd.posterior_mik)

set.seed(69)
samples_mik <- sample(p_grid, prob = prior_mik, size=1e4 , replace=TRUE)
plot(samples_mik)
dens(samples_mik)

# posterior predictive distribution
ppd_m <- rbinom( 1e4 , size=65 , prob=samples_mik)
simplehist(ppd_m, xlab = "mikkel predicted answers", col = blues9)

simplehist(ppd_m - 34) 
```

```{r all plots}
simplehist(ppd_r, xlab = "Posterior Prediction (Riccardo)", col = blues9[-(1:3)]) 
        abline(v = 9.08, col = "black", lty = 3, lwd = 2)

simplehist(ppd_k, xlab = "Posterior Prediction (Kristian)", col = blues9[-(1:3)])
        abline(v = 8.08, col = "black", lty = 3, lwd = 2)
           
simplehist(ppd_j, xlab = "Posterior Prediction (Josh)", col = blues9[-(1:3)])
        abline(v = 148.2, col = "black", lty = 3, lwd = 1.5)

simplehist(ppd_m, xlab = "Posterior Prediction (Mikkel)", col = blues9[-(1:3)])
        abline(v = 34.2, col = "black", lty = 3, lwd = 1.5)
```

Riccardo: With the prior questionnaire data our estimates of Riccardo’s knowledge turn out to be lower than how he performs on the new questionnaire. Despite the fact, that his estimated knowledge is increased by our prior (0.8), he outperforms our model predictions. Riccardo did his homework. 

Tylle: With the prior questionnaire data our estimates of Kristian’s knowledge turn out to be higher than how he performs on the new questionnaire. With the new data, Kristan no longer achieves “awesome knowledge” - it turns out he may not be as perfect as we thought. 

Josh: With the prior questionnaire data our estimates of Josh’s knowledge turn out to be lower than how he performs on the new questionnaire. Josh is really keeping up on new articles. He keeps getting better and it pays off, however it makes our model a bad predictor of him! 

Mikkel: With the prior questionnaire data our estimates of Mikkel’s knowledge turn out to be just slightly off compared to how he performs on the new questionnaire. We would have predicted a correct amount of answers close to chance level (33 correct) and Mikkel got 34 correct. He might not be the “best” of the teachers, but you always know what you get. 




